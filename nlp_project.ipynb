{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import time\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from typing import List, Dict, Tuple"
      ],
      "metadata": {
        "id": "8ExoFI3TkptW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9SlMPI3LeVL"
      },
      "outputs": [],
      "source": [
        "# Write the data_processor.py file\n",
        "%%writefile data_processor.py\n",
        "import json\n",
        "import random\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "\n",
        "class HotpotQADataset:\n",
        "    \"\"\"\n",
        "    Handles loading, filtering, and preparing HotpotQA data into the three\n",
        "    experimental conditions (Baseline, Noisy, Self-Correction) for the LLM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_path: str, sample_size: int = 100):\n",
        "        self.file_path = file_path\n",
        "        self.sample_size = sample_size\n",
        "        self.data = self._load_data()\n",
        "        self.experiment_data = self._select_multi_hop_sample()\n",
        "\n",
        "\n",
        "    def _load_data(self) -> List[Dict]:\n",
        "        \"\"\"Loads the JSON data file\"\"\"\n",
        "        print(f\"Loading data from {self.file_path}\")\n",
        "        with open(self.file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "\n",
        "\n",
        "    def _select_multi_hop_sample(self) -> List[Dict]:\n",
        "        \"\"\"Filters data for multi-hop questions and selects a manageable sample\"\"\"\n",
        "        filtered_data = [\n",
        "            item for item in self.data\n",
        "            if len(item.get('supporting_facts', [])) > 1\n",
        "               and item.get('type') == 'bridge'  # Focusing on 'bridge' Qs for simplicity\n",
        "        ]\n",
        "\n",
        "        # Randomly sample the desired size for the experiment\n",
        "        if len(filtered_data) > self.sample_size:\n",
        "            print(f\"Sampling {self.sample_size} multi-hop questions\")\n",
        "            return random.sample(filtered_data, self.sample_size)\n",
        "\n",
        "        print(f\"Using {len(filtered_data)} multi-hop questions for experiment.\")\n",
        "        return filtered_data\n",
        "\n",
        "\n",
        "    def _generate_contradictory_lie(self, supporting_fact: Tuple[str, int]) -> str:\n",
        "        \"\"\"Creates a plausible, contradictory lie based on one gold fact\"\"\"\n",
        "        fact_title, fact_id = supporting_fact\n",
        "\n",
        "        # For a simplified, reproducible example, the lie will be generated based on the fact's structure\n",
        "        lie_text = f\"Recent evidence indicates that the fact about '{fact_title}' is false.\"\n",
        "\n",
        "        return lie_text\n",
        "\n",
        "\n",
        "    def generate_inputs(self):\n",
        "        \"\"\"Generates the three prompt sets for all sampled questions\"\"\"\n",
        "        experiment_inputs = []\n",
        "        for item in self.experiment_data:\n",
        "            # Flatten the supporting facts into a single string of sentences\n",
        "            gold_facts_str = \" \".join([f\"Fact: {title}\" for title, _ in item['supporting_facts']])\n",
        "\n",
        "            # Select one fact to contradict\n",
        "            target_fact = item['supporting_facts'][1]\n",
        "            contradictory_lie = self._generate_contradictory_lie(target_fact)\n",
        "\n",
        "            # Condition A: Baseline (Factual)\n",
        "            prompt_A = f\"Context: {gold_facts_str}\\n\\nQuestion: {item['question']}\"\n",
        "\n",
        "            # Condition B: Noisy (Contradictory)\n",
        "            # The lie is injected into the context alongside the true facts\n",
        "            noisy_facts_str = f\"{gold_facts_str} {contradictory_lie}\"\n",
        "            prompt_B = f\"Context: {noisy_facts_str}\\n\\nQuestion: {item['question']}\"\n",
        "\n",
        "            # Condition C: Self-Correction\n",
        "            self_correction_prompt = (\n",
        "                \"\\n\\nTHINK CAREFULLY: Analyze all facts, including contradictions. \"\n",
        "                \"Check your assumptions and explain your final conclusion step-by-step.\"\n",
        "            )\n",
        "            prompt_C = prompt_B + self_correction_prompt\n",
        "\n",
        "            experiment_inputs.append({\n",
        "                'id': item['_id'],\n",
        "                'question': item['question'],\n",
        "                'gold_answer': item['answer'],\n",
        "                'target_fact': target_fact,\n",
        "                'lie': contradictory_lie,\n",
        "                'prompt_A': prompt_A,\n",
        "                'prompt_B': prompt_B,\n",
        "                'prompt_C': prompt_C,\n",
        "            })\n",
        "\n",
        "        print(f\"Generated inputs for {len(experiment_inputs)} questions.\")\n",
        "        return experiment_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nH6-Ex9E1CD"
      },
      "outputs": [],
      "source": [
        "# Writes the model_interface.py script\n",
        "%%writefile model_interface.py\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "\n",
        "class LLaMAEngine:\n",
        "    \"\"\"\n",
        "    Handles loading the Mistral model and executing inference for the\n",
        "    three experimental conditions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"mistralai/Mistral-7B-Instruct-v0.2\"):\n",
        "        self.model_name = model_name\n",
        "        self.device = self._get_device()\n",
        "        self.tokenizer = self._load_tokenizer()\n",
        "        self.model = self._load_model()\n",
        "\n",
        "\n",
        "    def _get_device(self) -> torch.device:\n",
        "        \"\"\"Determines the best device (GPU/CPU) to run the model\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            device = torch.device(\"cuda\")\n",
        "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        elif torch.backends.mps.is_available():\n",
        "            device = torch.device(\"mps\")\n",
        "            print(\"Using Apple Silicon (MPS) device.\")\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "            print(\"Using CPU (Inference will be slow).\")\n",
        "        return device\n",
        "\n",
        "\n",
        "    def _load_tokenizer(self) -> AutoTokenizer:\n",
        "        \"\"\"Loads the model's tokenizer\"\"\"\n",
        "        print(f\"Loading tokenizer: {self.model_name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        # Set pad token for generation stability\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        return tokenizer\n",
        "\n",
        "\n",
        "    def _load_model(self) -> AutoModelForCausalLM:\n",
        "        \"\"\"Loads the model to the determined device\"\"\"\n",
        "        print(f\"Loading model: {self.model_name}\")\n",
        "        # Use low_cpu_mem_usage=True and Bfloat16/float16 for efficiency\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.bfloat16 if self.device.type == 'cuda' else torch.float32,\n",
        "            low_cpu_mem_usage=True,\n",
        "            device_map=self.device\n",
        "        )\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "\n",
        "    def _generate_response(self, prompt: str) -> str:\n",
        "        \"\"\"Executes inference for a single prompt\"\"\"\n",
        "        # Format the prompt using the model's standard template\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        # Tokenize the input\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            return_tensors=\"pt\",\n",
        "            add_generation_prompt=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Generate the response\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(\n",
        "                input_ids,\n",
        "                max_new_tokens=256,  # Enough tokens for multi-step reasoning\n",
        "                do_sample=False,  # Set to False for reproducible answers\n",
        "                temperature=0.0,  # Low temperature for deterministic output\n",
        "                repetition_penalty=1.1  # Helps avoid repetition in reasoning steps\n",
        "            )\n",
        "\n",
        "        # Decode the output, skipping the original prompt tokens\n",
        "        response = self.tokenizer.decode(\n",
        "            output_ids[0, input_ids.shape[-1]:],\n",
        "            skip_special_tokens=True\n",
        "        ).strip()\n",
        "\n",
        "        return response\n",
        "\n",
        "\n",
        "    def run_experiment(self, input_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Runs the LLM against all three conditions for the entire dataset sample\"\"\"\n",
        "        results = []\n",
        "        print(f\"\\nRunning Experiment on {len(input_data)} Questions\")\n",
        "\n",
        "        for i, item in enumerate(input_data):\n",
        "            print(f\"Processing question {i + 1}/{len(input_data)}: {item['id']}\")\n",
        "\n",
        "            # Run Condition A (Baseline)\n",
        "            response_A = self._generate_response(item['prompt_A'])\n",
        "\n",
        "            # Run Condition B (Noisy)\n",
        "            response_B = self._generate_response(item['prompt_B'])\n",
        "\n",
        "            # Run Condition C (Self-Correction)\n",
        "            response_C = self._generate_response(item['prompt_C'])\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                'id': item['id'],\n",
        "                'question': item['question'],\n",
        "                'gold_answer': item['gold_answer'],\n",
        "                'lie_injected': item['lie'],\n",
        "                'response_A': response_A,\n",
        "                'response_B': response_B,\n",
        "                'response_C': response_C,\n",
        "                'target_fact': item['target_fact'],\n",
        "                # Note: Prompts (A, B, C) are stored here for full reproducibility\n",
        "                'prompt_A': item['prompt_A'],\n",
        "                'prompt_B': item['prompt_B'],\n",
        "                'prompt_C': item['prompt_C'],\n",
        "            })\n",
        "\n",
        "        print(\"Experiment run complete.\")\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3XTeBBH5P__"
      },
      "outputs": [],
      "source": [
        "# Writes the metrics.py script\n",
        "%%writefile metrics.py\n",
        "import re\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "\n",
        "def normalize_answer(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Standard QA normalization: lowercasing, removing articles, punctuation,\n",
        "    and extra whitespace to ensure consistent evaluation\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def calculate_factual_accuracy(gold_answer: str, model_response: str) -> float:\n",
        "    \"\"\"Determines if the model's response contains the true answer\"\"\"\n",
        "    gold_norm = normalize_answer(gold_answer)\n",
        "    response_norm = normalize_answer(model_response)\n",
        "\n",
        "    if gold_norm in response_norm or response_norm in gold_norm:\n",
        "        return 1.0\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def analyze_logical_coherence(\n",
        "    model_response: str,\n",
        "    target_fact: str,\n",
        "    lie_injected: str,\n",
        "    gold_answer: str\n",
        ") -> float:\n",
        "    \"\"\"Heuristic-based scoring (0-3) to evaluate the model's reasoning trace\"\"\"\n",
        "    lie_keywords = normalize_answer(lie_injected).split()[:3]\n",
        "    mentions_lie = any(keyword in normalize_answer(model_response) for keyword in lie_keywords)\n",
        "\n",
        "    factual_accuracy = calculate_factual_accuracy(gold_answer, model_response)\n",
        "\n",
        "    if factual_accuracy == 1.0:\n",
        "        if mentions_lie:\n",
        "            return 3.0\n",
        "        else:\n",
        "            return 2.0\n",
        "    else:\n",
        "        if mentions_lie:\n",
        "            return 1.0\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "def calculate_average_metrics(results: List[Dict[str, Any]]) -> Dict[str, float]:\n",
        "    \"\"\"Calculates the average factual and coherence scores across all three conditions\"\"\"\n",
        "    scores = {'A': {'factual': [], 'coherence': []},\n",
        "              'B': {'factual': [], 'coherence': []},\n",
        "              'C': {'factual': [], 'coherence': []}}\n",
        "\n",
        "    for item in results:\n",
        "        # Evaluate Condition A\n",
        "        scores['A']['factual'].append(\n",
        "            calculate_factual_accuracy(item['gold_answer'], item['response_A'])\n",
        "        )\n",
        "        scores['A']['coherence'].append(\n",
        "            2.0 if scores['A']['factual'][-1] == 1.0 else 0.0\n",
        "        )\n",
        "\n",
        "        # Evaluate Condition B\n",
        "        scores['B']['factual'].append(\n",
        "            calculate_factual_accuracy(item['gold_answer'], item['response_B'])\n",
        "        )\n",
        "        scores['B']['coherence'].append(\n",
        "            analyze_logical_coherence(item['response_B'], item['target_fact'],\n",
        "                                       item['lie_injected'], item['gold_answer'])\n",
        "        )\n",
        "\n",
        "        # Evaluate Condition C\n",
        "        scores['C']['factual'].append(\n",
        "            calculate_factual_accuracy(item['gold_answer'], item['response_C'])\n",
        "        )\n",
        "        scores['C']['coherence'].append(\n",
        "            analyze_logical_coherence(item['response_C'], item['target_fact'],\n",
        "                                       item['lie_injected'], item['gold_answer'])\n",
        "        )\n",
        "\n",
        "\n",
        "    # Average Calculation\n",
        "    avg_metrics = {}\n",
        "    for cond in ['A', 'B', 'C']:\n",
        "        factual_scores = scores[cond]['factual']\n",
        "        coherence_scores = scores[cond]['coherence']\n",
        "\n",
        "        # Ensures division by zero is avoided if a list is unexpectedly empty\n",
        "        avg_metrics[f'Avg_Factual_{cond}'] = sum(factual_scores) / len(factual_scores) if factual_scores else 0.0\n",
        "        avg_metrics[f'Avg_Coherence_{cond}'] = sum(coherence_scores) / len(coherence_scores) if coherence_scores else 0.0\n",
        "\n",
        "    return avg_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FniNJ17ESLO"
      },
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "DATA_URL = \"http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_dev_distractor_v1.json\"\n",
        "DATA_FILE_PATH = \"hotpot_dev_distractor_v1.json\"\n",
        "\n",
        "print(f\"Attempting direct download of {DATA_FILE_PATH}\")\n",
        "\n",
        "try:\n",
        "    response = requests.get(DATA_URL, stream=True)\n",
        "    response.raise_for_status()  # Check for HTTP errors\n",
        "\n",
        "    # Write the content chunk by chunk to prevent memory issues\n",
        "    with open(DATA_FILE_PATH, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    print(f\"File saved as {DATA_FILE_PATH} (Size: {os.path.getsize(DATA_FILE_PATH) / (1024*1024):.2f} MB)\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error during download: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J2VK3p_MAk7"
      },
      "outputs": [],
      "source": [
        "# Import from the files just created\n",
        "from data_processor import HotpotQADataset\n",
        "from model_interface import LLaMAEngine\n",
        "\n",
        "DATA_FILE_PATH = \"hotpot_dev_distractor_v1.json\"\n",
        "RESULTS_FILE_PATH = \"experiment_results.json\"\n",
        "EXPERIMENT_SAMPLE_SIZE = 50\n",
        "\n",
        "# Main Execution Function\n",
        "def run_full_experiment_colab():\n",
        "    \"\"\"Executes the entire pipeline and downloads results\"\"\"\n",
        "\n",
        "    # Data Preparation\n",
        "    print(\"Preparing Data and Generating Prompts\")\n",
        "    processor = HotpotQADataset(DATA_FILE_PATH, sample_size=EXPERIMENT_SAMPLE_SIZE)\n",
        "    input_data = processor.generate_inputs()\n",
        "\n",
        "    # Model Initialization\n",
        "    print(\"\\nInitializing LLaMA/Mistral Engine\")\n",
        "    engine = LLaMAEngine()\n",
        "\n",
        "    # Running the Experiment\n",
        "    start_time = time.time()\n",
        "    print(\"\\nRunning Inference on All Conditions (A, B, C) ---\")\n",
        "    results = engine.run_experiment(input_data)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Saving Results\n",
        "    print(\"\\nSaving and Downloading Results\")\n",
        "    with open(RESULTS_FILE_PATH, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "    print(f\"Experiment complete! Total time: {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    # Download the file\n",
        "    files.download(RESULTS_FILE_PATH)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_full_experiment_colab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjrsGD1HijEv"
      },
      "outputs": [],
      "source": [
        "from metrics import calculate_average_metrics\n",
        "\n",
        "# Set notebook visualization style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "# Configuration\n",
        "RESULTS_FILE_PATH = \"experiment_results.json\"\n",
        "\n",
        "# Check if the results file exists\n",
        "if not os.path.exists(RESULTS_FILE_PATH):\n",
        "    print(f\"Results file not found at '{RESULTS_FILE_PATH}'.\")\n",
        "    print(\"Please ensure you have downloaded experiment_results.json from Colab\")\n",
        "    raise FileNotFoundError\n",
        "\n",
        "# Load the experiment results\n",
        "with open(RESULTS_FILE_PATH, 'r', encoding='utf-8') as f:\n",
        "    results_data = json.load(f)\n",
        "\n",
        "# Convert results to a DataFrame for easier handling\n",
        "df = pd.DataFrame(results_data)\n",
        "print(f\"Loaded {len(df)} experimental results for analysis.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnv-OCBajJXE"
      },
      "outputs": [],
      "source": [
        "# Calculate the final average metrics\n",
        "avg_metrics = calculate_average_metrics(results_data)\n",
        "\n",
        "# Extract and format data for the summary table\n",
        "data_summary = {\n",
        "    'Condition': ['A: Baseline (Factual)', 'B: Noisy (Contradictory)', 'C: Self-Correction (Treated)'],\n",
        "    'Avg. Factual Accuracy': [\n",
        "        avg_metrics['Avg_Factual_A'],\n",
        "        avg_metrics['Avg_Factual_B'],\n",
        "        avg_metrics['Avg_Factual_C']\n",
        "    ],\n",
        "    'Avg. Logical Coherence (0-3)': [\n",
        "        avg_metrics['Avg_Coherence_A'],\n",
        "        avg_metrics['Avg_Coherence_B'],\n",
        "        avg_metrics['Avg_Coherence_C']\n",
        "    ]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(data_summary).set_index('Condition')\n",
        "\n",
        "print(\"\\nExperimental Results\")\n",
        "print(metrics_df.round(3))\n",
        "\n",
        "# Calculate the causal effects\n",
        "deterioration = metrics_df.loc['A: Baseline (Factual)', 'Avg. Factual Accuracy'] - metrics_df.loc['B: Noisy (Contradictory)', 'Avg. Factual Accuracy']\n",
        "recovery = metrics_df.loc['C: Self-Correction (Treated)', 'Avg. Factual Accuracy'] - metrics_df.loc['B: Noisy (Contradictory)', 'Avg. Factual Accuracy']\n",
        "\n",
        "print(f\"\\nCausal Effects\")\n",
        "print(f\"Deterioration due to Lie (A vs B): {deterioration:.3f}\")\n",
        "print(f\"Recovery due to Self-Correction Prompt (C vs B): {recovery:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIcwyZe9Nk0u"
      },
      "outputs": [],
      "source": [
        "# Prepare data for plotting\n",
        "plot_data = metrics_df.reset_index().melt(\n",
        "    id_vars='Condition',\n",
        "    value_vars=['Avg. Factual Accuracy', 'Avg. Logical Coherence (0-3)'],\n",
        "    var_name='Metric',\n",
        "    value_name='Score'\n",
        ")\n",
        "\n",
        "# Factual Accuracy Comparison\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.barplot(x='Condition', y='Score', data=plot_data[plot_data['Metric'] == 'Avg. Factual Accuracy'], palette=\"viridis\")\n",
        "plt.title('Effect of Contradiction and Self-Correction on Factual Accuracy')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.ylabel('Average Factual Accuracy (0-1)')\n",
        "plt.xticks(rotation=15)\n",
        "plt.show()\n",
        "\n",
        "# Logical Coherence Comparison\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.barplot(x='Condition', y='Score', data=plot_data[plot_data['Metric'] == 'Avg. Logical Coherence (0-3)'], palette=\"plasma\")\n",
        "plt.title('Effect of Self-Correction on Logical Coherence')\n",
        "plt.ylim(0, 3.0)\n",
        "plt.ylabel('Average Logical Coherence Score (0-3)')\n",
        "plt.xticks(rotation=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Opwm9N0TNn2W"
      },
      "outputs": [],
      "source": [
        "# Re-apply the individual scoring logic to the DataFrame to find specific cases\n",
        "from metrics import calculate_factual_accuracy, analyze_logical_coherence\n",
        "\n",
        "# Add individual scores back to the DataFrame\n",
        "df['acc_A'] = df.apply(lambda row: calculate_factual_accuracy(row['gold_answer'], row['response_A']), axis=1)\n",
        "df['coh_B'] = df.apply(lambda row: analyze_logical_coherence(row['response_B'], row['target_fact'], row['lie_injected'], row['gold_answer']), axis=1)\n",
        "df['coh_C'] = df.apply(lambda row: analyze_logical_coherence(row['response_C'], row['target_fact'], row['lie_injected'], row['gold_answer']), axis=1)\n",
        "\n",
        "\n",
        "# Successful Self-Correction (Target Success Case)\n",
        "# Find a case where the model failed the lie (coh_B < 2) but the prompt fixed it (coh_C > 2)\n",
        "success_case = df[(df['coh_B'] < 2) & (df['coh_C'] >= 2)].iloc[0]\n",
        "\n",
        "print(\"\\nCase A: SUCCESSFUL Self-Correction (Prompt Triumphs over Lie)\")\n",
        "print(f\"Question ID: {success_case['id']}\")\n",
        "print(f\"Question: {success_case['question']}\")\n",
        "print(f\"Lie Injected: {success_case['lie_injected']}\")\n",
        "print(f\"Gold Answer: {success_case['gold_answer']}\")\n",
        "print(\"\\n[B] Response (Failed Lie Test):\")\n",
        "print(success_case['response_B'])\n",
        "print(\"\\n[C] Response (Successful Self-Correction):\")\n",
        "print(success_case['response_C'])\n",
        "\n",
        "\n",
        "# Prompt Failure (The Lie is Too Strong)\n",
        "# Find a case where the model fell for the lie AND the prompt didn't help (coh_C is low)\n",
        "failure_case = df[(df['coh_B'] <= 1) & (df['coh_C'] <= 1)].iloc[0]\n",
        "\n",
        "print(\"\\nCase B: PROMPT FAILURE (Lie Persistence)\")\n",
        "print(f\"Question ID: {failure_case['id']}\")\n",
        "print(f\"Question: {failure_case['question']}\")\n",
        "print(f\"Lie Injected: {failure_case['lie_injected']}\")\n",
        "print(f\"Gold Answer: {failure_case['gold_answer']}\")\n",
        "print(\"\\n[C] Response (Despite Self-Correction Prompt):\")\n",
        "print(failure_case['response_C'])\n",
        "\n",
        "# Save the DataFrame with scores for the repository (optional but recommended)\n",
        "df.to_csv(\"detailed_results_with_scores.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3mn8Vo7LdqY"
      },
      "outputs": [],
      "source": [
        "def analyze_doubt_confidence(model, tokenizer, prompt, max_new=15):\n",
        "    \"\"\"Analyzes the model's confidence scores for a specific reasoning trace.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            temperature=0.0 # Maintain consistency with your original study\n",
        "        )\n",
        "\n",
        "    # Calculate log-probabilities\n",
        "    transition_scores = model.compute_transition_scores(\n",
        "        outputs.sequences, outputs.scores, normalize_logits=True\n",
        "    )\n",
        "\n",
        "    # Decode tokens and calculate confidence (0% to 100%)\n",
        "    tokens = outputs.sequences[0][len(inputs.input_ids[0]):]\n",
        "    for tok, score in zip(tokens, transition_scores[0]):\n",
        "        token_text = tokenizer.decode(tok)\n",
        "        confidence = torch.exp(score).item() * 100\n",
        "        print(f\"Token: '{token_text}' | Confidence: {confidence:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wpO9ZBOLsM8"
      },
      "outputs": [],
      "source": [
        "with open('experiment_results.json', 'r') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "# Filter for Condition C cases that resulted in 'uncertainty'\n",
        "doubt_cases = []\n",
        "for r in results:\n",
        "    # Check if 'response_C' exists and contains keywords of uncertainty\n",
        "    res_c = r.get('response_C', '').lower()\n",
        "    if any(word in res_c for word in ['uncertain', 'investigation', 'impossible to determine', 'cannot definitively']):\n",
        "        doubt_cases.append(r)\n",
        "\n",
        "print(f\"Found {len(doubt_cases)} Doubt cases to analyze.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyE4rMsfLwtK"
      },
      "outputs": [],
      "source": [
        "engine = LLaMAEngine()\n",
        "\n",
        "# Analyze the first detected Doubt Loop case\n",
        "if doubt_cases:\n",
        "    case = doubt_cases[0]\n",
        "    print(f\"Analyzing Case: {case['question']}\")\n",
        "    # You use the same prompt template you used for Condition C\n",
        "    analyze_doubt_confidence(engine.model, engine.tokenizer, case['prompt_C'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}